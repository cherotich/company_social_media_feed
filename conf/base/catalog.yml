# Here you can define all your data sets by using simple YAML syntax.
#
# Documentation for this file format can be found in "The Data Catalog"
# Link: https://kedro.readthedocs.io/en/stable/data/data_catalog.html

<<<<<<< HEAD

raw_data:
  type: pandas.ParquetDataSet
  filepath: data/01_raw/raw.parquet
  layer: raw
  load_args:
    engine: pyarrow
    use_nullable_dtypes: True
  save_args:
    engine: pyarrow
    compression: GZIP

cleaned_data:
  type: pandas.ParquetDataSet
  filepath: data/02_intermediate/cleaned_data.parquet
  layer: intermediate
  load_args:
    engine: pyarrow
    use_nullable_dtypes: True
  save_args:
    engine: pyarrow
    compression: GZIP

labelled_data:
  type: pandas.ParquetDataSet
  filepath: data/07_model_output/model_output.parquet
  layer: model_output
  load_args:
    engine: pyarrow
    use_nullable_dtypes: True
  save_args:
    engine: pyarrow
    compression: GZIP






=======
company_data:
  type: spark.SparkDataSet
  filepath: data/01_raw/train.csv
  file_format: csv
  #credentials: dev_s3
  load_args:
    header: True
    inferSchema: True
  save_args:
    sep: '\t'

model_input_data:
  type: MemoryDataSet

twitter_analytics.dummy_data:
  type: MemoryDataSet

text_comprehension.dummy_data:
  type: MemoryDataSet

text_classification.dummy_data:
  type: MemoryDataSet


# dummy_data:
#   type: spark.SparkDataSet
  # filepath: data/01_raw/osdg-dummy_data.csv
  # file_format: csv
  # #credentials: dev_s3
  # load_args:
  #   header: True
  #   inferSchema: True
  # save_args:
  #   sep: '\t'
  #   overwrite: True
>>>>>>> 136b2c5d3f082e71849c746b59257576b6b1e43c
